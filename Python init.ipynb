{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# !pip install -r requirements.txt",
   "id": "54a2850d1a340767"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import subprocess\n",
    "import shutil"
   ],
   "id": "8377a320a42284f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Clone the repository\n",
    "subprocess.run([\"git\", \"clone\", \"https://huggingface.co/datasets/dreamerdeo/multispider\"], check=True)\n",
    "\n",
    "# Move the directories\n",
    "shutil.move(\"multispider/dataset\", \".\")\n",
    "shutil.move(\"multispider/model\", \".\")"
   ],
   "id": "13be5ff40ce68385"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Prepare the dataset\n",
    "\n",
    "We're gonna separate the dataset into the folder of the different databases, to reduce the size of the dataset when working a given database.\n",
    "\n",
    "We will keep the distinction between dev, train and examples sets"
   ],
   "id": "db04c232df7efde8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T10:31:37.193570Z",
     "start_time": "2024-10-03T10:31:37.190451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_path = r\"../multispider/multispider/dataset/multispider/with_original_value\"\n",
    "database_path = r\"../multispider/multispider/dataset/spider/database\""
   ],
   "id": "d3c4030934299cde",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "On crée une liste des databases\n",
    "\n",
    "Pour chaque fichier dans la liste suivante [dev_fr.json, train_fr.json] on effectue le traitement suivant:\n",
    "- On ouvre le fichier\n",
    "- on crée un dictionnaire avec comme clefs les noms des bases de données et une chaine de caractère vide comme valeur\n",
    "- On parcourt les lignes du fichier Json en cherchant la partie \"db_id\"\n",
    "- On ajoute la ligne dans le dictionnaire correspondant à la base de données\n",
    "- Puis une fois le fichier parcouru, on écrit les lignes dans un nouveau fichier json dans le dossier correspondant à la base de données pour chaque base de données dans le dictionnaire, en conservant le nom du fichier original\n",
    "\n"
   ],
   "id": "7f30876e96c3524d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T12:54:06.265219Z",
     "start_time": "2024-10-03T12:54:06.244889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def extract_database(file_path):\n",
    "    with open(os.path.join(dataset_path, file_path), \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "        databases = {}\n",
    "        for line in data:\n",
    "            db_id = line[\"db_id\"]\n",
    "            if db_id not in databases:\n",
    "                databases[db_id] = []\n",
    "            databases[db_id].append(line)\n",
    "    return databases\n",
    "\n",
    "def write_database(databases, file_path):\n",
    "    for db_id, lines in databases.items():\n",
    "        with open(os.path.join(database_path, db_id, file_path), \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(lines, file)\n",
    "            \n",
    "def extract_and_write_database(file_path):\n",
    "    databases = extract_database(file_path)\n",
    "    write_database(databases, file_path)\n",
    "    \n",
    "def extract_all_databases():\n",
    "    for file in [\"dev_fr.json\", \"train_fr.json\"]:\n",
    "        extract_and_write_database(file)\n",
    "        "
   ],
   "id": "2236ea5f0d926c49",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T12:54:10.579256Z",
     "start_time": "2024-10-03T12:54:08.411944Z"
    }
   },
   "cell_type": "code",
   "source": "extract_all_databases()",
   "id": "be8f20d7dc599a1c",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T11:48:24.103348Z",
     "start_time": "2024-10-03T11:48:23.600955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# On fait la liste de toutes les bases de données avec un fichier train_fr.json, et celles avec un fichier dev_fr.json\n",
    "\n",
    "def list_databases(file_path):\n",
    "    with open(os.path.join(dataset_path, file_path), \"r\") as file:\n",
    "        data = json.load(file)\n",
    "        databases = set()\n",
    "        for line in data:\n",
    "            db_id = line[\"db_id\"]\n",
    "            databases.add(db_id)\n",
    "    return databases\n",
    "\n",
    "def list_all_databases():\n",
    "    databases_dev = list_databases(\"dev_fr.json\")\n",
    "    databases_train = list_databases(\"train_fr.json\")\n",
    "    return databases_dev, databases_train\n",
    "\n",
    "databases_dev, databases_train = list_all_databases()"
   ],
   "id": "10488a2d4cfceed3",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T11:48:38.575074Z",
     "start_time": "2024-10-03T11:48:38.557926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# on cherche les databases qui sont dans les deux listes\n",
    "databases_dev & databases_train"
   ],
   "id": "95541aa8d9804151",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T11:58:58.785602Z",
     "start_time": "2024-10-03T11:58:58.765005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# On enregistre les bases de données dans des fichiers JSON\n",
    "with open(\"databases_dev.json\", \"w\") as file:\n",
    "    json.dump(list(databases_dev), file)\n",
    "\n",
    "with open(\"databases_train.json\", \"w\") as file:\n",
    "    json.dump(list(databases_train), file)"
   ],
   "id": "3c62706f0833cdfb",
   "outputs": [],
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
